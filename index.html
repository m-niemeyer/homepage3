<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>Michael  Niemeyer</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.xyz/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ðŸ¤–</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">

<link rel="canonical" href="/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

  <script src="/assets/js/theme.js"></script>
  <!-- Load DarkMode JS -->
<script src="/assets/js/dark_mode.js"></script>






    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.1.2/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav sticky-bottom-footer">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Blog -->
          <li class="nav-item ">
            <a class="nav-link" href="/blog/">
              blog
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/projects/">
                projects
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/talks/">
                talks
                
              </a>
          </li>
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li>
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     <span class="font-weight-bold">Michael</span>  Niemeyer
    </h1>
     <p class="desc">PhD Student in Computer Vision / Machine Learning</p>
  </header>

  <article>
    
    <div class="profile float-right">
      
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/prof_pic.jpg">
      
      
    </div>
    

    <div class="clearfix">
      <p>Iâ€™m Michael, a PhD student in the field of computer vision / machine learning in the <a href="https://uni-tuebingen.de/en/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/autonomous-vision/home/" target="_blank">AVG group</a> at the <a href="https://uni-tuebingen.de/en/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/autonomous-vision/home/" target="_blank">Max Planck Insitute for Intelligent Systems</a>.</p>

<p>My research focuses on 3D vision. I am interested in how machines can infer 3D representations from sparse observations. Further, I am big enthusiast of neural scene representations. I currently investigate how scenes are best represented for machines using deep neural networks.</p>

<p>I studied BSc Mathematics at the <a href="http://www.mi.uni-koeln.de/main/index.en.php" target="_blank">University of Cologne</a>, Germany. During my Bachelor studies, I spent one year at the <a href="https://www.ub.edu/web/portal/en/" target="_blank">University of Barcelona</a>, Spain, funded by the ERASMUS program.
Following my interest in computer science, I then moved to Scotland to gain my Masterâ€™s degree in Advanced Computer Science at the <a href="https://www.st-andrews.ac.uk/computer-science/" target="_blank">University of St Andrews</a>.
In October 2018, I started my PhD in computer vision / machine learning in the <a href="https://uni-tuebingen.de/en/fakultaeten/mathematisch-naturwissenschaftliche-fakultaet/fachbereiche/informatik/lehrstuehle/autonomous-vision/home/" target="_blank">AVG group</a> at the <a href="https://is.mpg.de/" target="_blank">Max Planck Institute for Intelligent Systems</a> in TÃ¼bingen, Germany, under the supervision of <a href="http://cvlibs.net/" target="_blank">Andreas Geiger</a>.</p>

<p>Feel free to reach out to me via e-mail for any inquiries!</p>

<p><a href="mailto:michael.niemeyer@tue.mpg.de" style="margin-right: 15px"><i class="far fa-envelope-open fa-lg"></i> Mail</a>
<a href="https://twitter.com/Mi_Niemeyer" target="_blank" style="margin-right: 15px"><i class="fab fa-twitter fa-lg"></i> Twitter</a>
<a href="https://scholar.google.com/citations?user=v1O7i_0AAAAJ&amp;hl=en" target="_blank" style="margin-right: 15px"><i class="ai ai-google-scholar ai-lg"></i> Scholar</a>
<a href="https://github.com/m-niemeyer" target="_blank" style="margin-right: 15px"><i class="fab fa-github fa-lg"></i> Github</a>
<a href="https://www.linkedin.com/in/michael-niemeyer" target="_blank" style="margin-right: 15px"><i class="fab fa-linkedin fa-lg"></i> LinkedIn</a>
<a href="https://avg.is.tuebingen.mpg.de/person/mniemeyer" target="_blank"><i class="fas fa-briefcase fa-lg"></i> Work</a></p>

    </div>

    
      <div class="news">
  <h2>news</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
        <tr>
          <th scope="row">Mar 31, 2021</th>
          <td>
            
              We released a new preprint <a href="https://arxiv.org/abs/2103.17269" target="_blank">CAMPARI: Camera-Aware Decomposed Generative Neural Radiance Fields</a>

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Feb 28, 2021</th>
          <td>
            
              Our <a href="https://arxiv.org/abs/2011.12100" target="_blank">GIRAFFE</a> paper is accepted to CVPRâ€™21 as an oral

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jan 13, 2021</th>
          <td>
            
              I was invited to give a talk the <a href="https://www.youtube.com/watch?v=scnXyCSMJF4" target="_blank">MIT seminar series on 3D representations</a>

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Nov 30, 2020</th>
          <td>
            
              We released a new preprint: <a href="https://arxiv.org/abs/2011.12100" target="_blank">GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields</a>

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Oct 1, 2020</th>
          <td>
            
              Our paper <a href="https://arxiv.org/abs/2003.12406" target="_blank">Implicit Surface Light Field Paper</a> is accepted to 3DVâ€™20

            
          </td>
        </tr>
      
      </table>
    </div>
  
</div>

    

    
      <div class="publications">
  <h2>selected publications</h2>
  <p>Please see the <a href="/publications/" target="_blank">publications tab</a> or <a href="https://scholar.google.com/citations?user=v1O7i_0AAAAJ&hl=en" target="_blank">Google Scholar</a> for a complete list.</p>
  <ol class="bibliography"><li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid rounded z-depth-1" src="/assets/img/publications/onet.jpg" alt=""/>
  
  </div>

  <div id="Mescheder2019CVPR" class="col-sm-7">
    
      <div class="title">Occupancy Networks: Learning 3D Reconstruction in Function Space</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                
                  Mescheder, Lars,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Oechsle, Michael,
                
              
            
          
        
          
          
          
          
          
          
            
              
                <em>Niemeyer, Michael</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Nowozin, Sebastian,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Geiger, Andreas
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. IEEE Conf. on Computer Vision and Pattern	Recognition (CVPR)</em>
      
      
        2019
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>With the advent of deep neural networks, learning-based approaches for 3DÂ reconstruction have gained popularity. However, unlike for images, in 3D there is no canonical representation which is both computationally and memory efficient yet allows for representing high-resolution geometry of arbitrary topology. Many of the state-of-the-art learning-based 3DÂ reconstruction approaches can hence only represent very coarse 3D geometry or are limited to a restricted domain. In this paper, we propose Occupancy Networks, a new representation for learning-based 3DÂ reconstruction methods. Occupancy networks implicitly represent the 3D surface as the continuous decision boundary of a deep neural network classifier. In contrast to existing approaches, our representation encodes a description of the 3D output at infinite resolution without excessive memory footprint. We validate that our representation can efficiently encode 3D structure and can be inferred from various kinds of input. Our experiments demonstrate competitive results, both qualitatively and quantitatively, for the challenging tasks of 3D reconstruction from single images, noisy point clouds and coarse discrete voxel grids. We believe that occupancy networks will become a useful tool in a wide variety of learning-based 3D tasks.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid rounded z-depth-1" src="/assets/img/publications/dvr.jpg" alt=""/>
  
  </div>

  <div id="Niemeyer2020CVPR" class="col-sm-7">
    
      <div class="title">Differentiable Volumetric Rendering: Learning Implicit 3D Representations without 3D Supervision</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Niemeyer, Michael</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Mescheder, Lars,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  Oechsle, Michael,
                
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Geiger, Andreas
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. IEEE Conf. on Computer Vision and Pattern	Recognition (CVPR)</em>
      
      
        2020
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
    
      
      <a href="http://www.cvlibs.net/publications/Niemeyer2020CVPR.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">PDF</a>
      
    
    
      
      <a href="http://www.cvlibs.net/publications/Niemeyer2020CVPR_supplementary.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank">Supp</a>
      
    
    
      <a href="https://autonomousvision.github.io/differentiable-volumetric-rendering" class="btn btn-sm z-depth-0" role="button" target="_blank">Blog</a>
    
    
      <a href="https://github.com/autonomousvision/differentiable_volumetric_rendering" class="btn btn-sm z-depth-0" role="button" target="_blank">Code</a>
    
    
    
    
    
    <a href="https://www.youtube.com/watch?v=U_jIN3qWVEw" class="btn btn-sm z-depth-0" role="button" target="_blank">Video</a>
  
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Learning-based 3D reconstruction methods have shown impressive results. However, most methods require 3D supervision which is often hard to obtain for real-world datasets. Recently, several works have proposed differentiable rendering techniques to train reconstruction models from RGB images. Unfortunately, these approaches are currently restricted to voxel- and mesh-based representations, suffering from discretization or low resolution. In this work, we propose a differentiable rendering formulation for implicit shape and texture representations. Implicit representations have recently gained popularity as they represent shape and texture continuously. Our key insight is that depth gradients can be derived analytically using the concept of implicit differentiation. This allows us to learn implicit shape and texture representations directly from RGB images. We experimentally show that our single-view reconstructions rival those learned with full 3D supervision. Moreover, we find that our method can be used for multi-view 3D reconstruction, directly resulting in watertight meshes.</p>
    </div>
    
  </div>
</div>
</li>
<li><div class="row">
  <div class="col-sm-3">
  
    <img class="img-fluid rounded z-depth-1" src="/assets/img/publications/giraffe.jpg" alt=""/>
  
  </div>

  <div id="Niemeyer2021CVPR" class="col-sm-7">
    
      <div class="title">GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields</div>
      <div class="author">
        
          
          
          
          
          
          
            
              
                <em>Niemeyer, Michael</em>,
              
            
          
        
          
          
          
          
          
          
            
              
                
                  and Geiger, Andreas
                
              
            
          
        
      </div>

      <div class="periodical">
      
        <em>In Proc. IEEE Conf. on Computer Vision and Pattern	Recognition (CVPR)</em>
      
      
        2021
      
      </div>
    

    <div class="links">
    
      <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
    
    
    
      <a href="https://m-niemeyer.github.io/project-pages/giraffe/index.html" class="btn btn-sm z-depth-0" role="button" target="_blank">HTML</a>
    
    
    
    
    
    
    
    
    
    </div>

    <!-- Hidden abstract block -->
    
    <div class="abstract hidden">
      <p>Deep generative models allow for photorealistic image synthesis at high resolutions. But for many applications, this is not enough: content creation also needs to be controllable. While several recent works investigate how to disentangle underlying factors of variation in the data, most of them operate in 2D and hence ignore that our world is three-dimensional. Further, only few works consider the compositional nature of scenes. Our key hypothesis is that incorporating a compositional 3D scene representation into the generative model leads to more controllable image synthesis. Representing scenes as compositional generative neural feature fields allows us to disentangle one or multiple objects from the background as well as individual objectsâ€™ shapes and appearances while learning from unstructured and unposed image collections without any additional supervision. Combining this scene representation with a neural rendering pipeline yields a fast and realistic image synthesis model. As evidenced by our experiments, our model is able to disentangle individual objects and allows for translating and rotating them in the scene as well as changing the camera pose.</p>
    </div>
    
  </div>
</div>
</li></ol>
</div>

    

    
    <div class="social">
      <div class="contact-icons">
        <a href="mailto:%6D%69%63%68%61%65%6C.%6E%69%65%6D%65%79%65%72@%74%75%65.%6D%70%67.%64%65"><i class="fas fa-envelope"></i></a>

<a href="https://scholar.google.com/citations?user=v1O7i_0AAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>


<a href="https://github.com/m-niemeyer" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
<a href="https://www.linkedin.com/in/michael-niemeyer" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
<a href="https://twitter.com/Mi_Niemeyer" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>



<a href="https://avg.is.tuebingen.mpg.de/person/mniemeyer" target="_blank" title="Work"><i class="fas fa-briefcase"></i></a>





      </div>
      <div class="contact-note">For any inquiries, please contact me via e-mail. 
</div>
    </div>
    
  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="sticky-bottom mt-5">
  <div class="container">
    &copy; Copyright 2021 Michael  Niemeyer.
    Created using the <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.
    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
